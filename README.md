## Deep Learning Reading Log (cpsc 552)

### Reading List:
***
#### Variations on SGD: <br/>
[The Loss Surfaces of Multilayer Networks](https://arxiv.org/abs/1412.0233) <br/>
[Adam](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c) <br/>
***
#### Unsupervised Learning and Autoencoders: <br/>
Hinton and Salakhutdinov Science 2006 <br/>
Coifman and Lafon Applied and Computational Harmonic Analysis 2006 <br/>
Belkin & Niyogi, Neural Computation 2003 <br/>
Alain & Bengio, JMLR 2014 <br/>
***
#### Generative Models and VAEs: <br/>
Kingma & Welling ICLR 2014 <br/>
Dziugaite et al.  UAI 2015 <br/>
Bengio et al. NeurIPS 2013 <br/>
Lopez et al. Nature Methods 2018 <br/>
***
#### GANs: <br/>
[From GAN to WGAN](https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html) <br/>
[Maximizing likelihood is equivalent to minimizing KL-Divergence](https://agustinus.kristia.de/techblog/2017/01/26/kl-mle/) <br/>
Goodfellow et al 2014 paper <br/>
Arjovsky et al 2017 paper <br/>
[StyleGAN](https://arxiv.org/pdf/1812.04948.pdf) <br/>
[InfoGAN](https://arxiv.org/pdf/1606.03657.pdf) <br/>
[DiscoGAN](https://arxiv.org/abs/1703.05192) <br/>
[CycleGAN](https://arxiv.org/abs/1703.10593) <br/>
[MAGAN](http://proceedings.mlr.press/v80/amodio18a.html) <br/>
[TraVeLGAN](https://arxiv.org/abs/1902.09631) <br/>
[Conditional GAN](https://arxiv.org/pdf/1411.1784.pdf) <br/>
***
#### CNNs: <br/>
[Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/convolutional-networks/) <br/>
Szegedy et al. Going deeper with convolutions 2014 <br/>
He et al. Deep Residual Learning for Image Recognition 2015 <br/>
Ronneberger et al.  U-Net: Convolutional Networks for Biomedical Image Segmentation 2014 <br/>
[An Intuitive Explanation of Convolutional Neural Networks](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/) <br/>
[A Simple Guide to the Versions of the Inception Network](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202) <br/>
***
#### RNNs & LSTMs: <br/>
[word2vec](https://arxiv.org/abs/1301.3781) <br/>
[Luong et al. 2015](https://arxiv.org/abs/1508.04025) <br/>
[Hochreiter and Schmidhuber. 1997](https://www.bioinf.jku.at/publications/older/2604.pdf) <br/>
[Attention in RNNs blog](https://medium.datadriveninvestor.com/attention-in-rnns-321fbcd64f05) <br/>
[Understanding LSTMs blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) <br/>
***
#### Transformers: <br/>
[Attention and its Different Forms](https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc) <br/>
[Illustrated Transformers](http://jalammar.github.io/illustrated-transformer/) <br/>
[Attention is All You Need](https://arxiv.org/abs/1706.03762) <br/>
Radford et al. (GPT2 paper) <br/>
Brown et al. (GPT3 paper) <br/>
Child et al. (Sparse Transformer) <br/>
[Illustrateed GPT2](https://jalammar.github.io/illustrated-gpt2 (Links to an external site.)/) <br/>
[The Journey of OpenAI GPT Models](https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2) <br/>
***
#### Graph Neural Networks: <br/>
Defferrard et al. Convolutional Neural Networks on Graphs <br/>
Bruna et al. Spectral Networks and Locally Connected Networks on Graphs <br/>
Hamilton et al. [Inductive Representation Learning on Large Graphs](https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf) <br/>
Perozzi et al. Deep Walk <br/>
Kipf & Welling, Semisupervised Graph Classifiation <br/>
[Spectral Graph Convolution Explained and Implemented Step by Step](https://towardsdatascience.com/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801) <br/>
[Anisotropic, Dynamic, Spectral and Multiscale Filters Defined on Graphs](https://towardsdatascience.com/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49) <br/>
[A Gentle Introduction to Graph Neural Networks (Basics, DeepWalk, and GraphSage)](https://towardsdatascience.com/a-gentle-introduction-to-graph-neural-network-basics-deepwalk-and-graphsage-db5d540d50b3) <br/>
Gao et al., ICML 2019 [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e/gao19e.pdf) <br/>
Min et al. NeurIPS 2020 [Scattering GCN: Overcoming Oversmoothness in Graph Convolutional Networks](https://arxiv.org/pdf/2003.08414.pdf) <br/>
Min et al. [Geometric Scattering Attention Networks](https://arxiv.org/abs/2010.15010)
***
#### Neural ODEs: <br/>
Chen et al. NeurIPS 2019 Neural Ordinary Differential Equations <br/>
Tong et al. ICML 2020 TrajectoryNet <br/>
Errico 1997 Adjoint Model <br/>
[The Story of Adjoint Sensitivity Method from Meteorology](https://towardsdatascience.com/the-story-of-adjoint-sensitivity-method-from-meteorology-906ab2796c73) <br/>
[Neural ODEs: breakdown of another deep learning breakthrough](https://towardsdatascience.com/neural-odes-breakdown-of-another-deep-learning-breakthrough-3e78c7213795) <br/>
***
#### Universality of Neural Networks: <br/>
DasGupta & Gupta 2002 Johnson <br/>
[An Elementary Proof of a Theorem of Johnson and Lindenstrauss](https://cseweb.ucsd.edu/~dasgupta/papers/jl.pdf) <br/>
Frankle & Carbin 2019 [THE LOTTERY TICKET HYPOTHESIS: FINDING SPARSE, TRAINABLE NEURAL NETWORKS](https://arxiv.org/pdf/1803.03635.pdf) <br/>
Keskar et al. 2017 ON LARGE-BATCH TRAINING FOR DEEP LEARNING: GENERALIZATION GAP AND SHARP MINIMA <br/>
LeCun et al. 1990 Optimal Brain Damage <br/>
Draxler et al. 2019 Essentially No Barriers in Neural Network Energy Landscape <br/>
Li et al. 2018 Visualizing the Loss Landscape of Neural Nets <br/>
Choromanska et al. 2015 Loss Surfaces of Multilayer Neural Networks <br/>
Tishby. et al. Information Bottleneck Paper <br/>
Gigante et al. M-PHATE Paper <br/>
Goldfeld et al. Information Flow <br/>
***
#### Generalization and Memorization: <br/>
Belkin, et al. 2019 PNAS Reconciling Modern Machine-Learning Practice and The Classical Bias-Variance Tradeoff <br/>
[Understanding Deep Learning requires rethinking generalization](https://arxiv.org/pdf/1611.03530.pdf), Zhang et al ICLR 2017 <br/>
[A closer look at memorization in deep networks](https://arxiv.org/pdf/1706.05394.pdf) Arpit et al 2017 <br/>
[In search of the real inductive bias: on the role of implicit regularization in deep learning](https://arxiv.org/pdf/1412.6614.pdf) Neyshabur et al, 2015 <br/>
[The role of over-parametrization in generalization of neural networks](https://arxiv.org/pdf/1805.12076.pdf) Neyshabur et al, 2019 <br/>
[Train faster, generalize better: stability of stochastic gradient descent](https://arxiv.org/pdf/1509.01240.pdf) Hardt et al, 2016 <br/>
[Stability and generalization](https://www.academia.edu/13743279/Stability_and_generalization) Bousquet and A. Elisseeff 2002 <br/>
[Rademacher complexity](http://www.cs.cmu.edu/~ (Links to an external site.)ninamf/ML11/lect1117.pdf) <br/>
Chatterjee S. ICLR 2020 Coherent Gradients: An approach to understanding generalization in gradient descent-based optimization <br/>
***
#### Neural Tanget Kernels: <br/>
Jacot et al. Neural Tangent Kernel: Convergence and Generalization in Neural Networks, NeurIPS 2018 <br/>
Chizat et al. On Lazy Training in Differentiable Programming NeurIPS 2019 <br/>
Arora, Sanjeev, et al. On exact computation with an infinitely wide neural net NeurIPS 2019 <br/>
Li, Zhiyuan, et al. Enhanced Convolutional Neural Tangent Kernels NeurIPS 2019 <br/>
[Kernel Functions](https://towardsdatascience.com/kernel-function-6f1d2be6091) <br/>
[Understanding the Neural Tangent Kernel](https://rajatvd.github.io/NTK/) <br/>

## Deep Learning with D2L Materials

Learning Log:

_12_24_2021_ - preliminaries: torch basics (2) <br/>
_12_25_2021_ - CNN: convolutions, padding, stride, multiple channels, pooling, LeNet (6) <br/>
_12_26_2021_ - RNN, language modeling concepts (8.1-8.4) <br/>
_12_27_2021_ - RNN from scratch, PyTorch RNN (8.5-8.7) <br/>
_12_28_2021_ - modern RNNS: GRU, LSTM, bidirectional (9) <br/>
_12_29_2021_ - Attention (10) <br/>
_12_30_2021_ - Linear NN: linear regression, softmax regression (3) <br/>
_12_31_2021_ - Multilayer perceptron regularization (4) <br/>
_01_01_2022_ - Optimization algorithms, learning rate scheduling (11) <br/>
_01_02_2022_ - deep learning computation, layers, blocks, I/O, GPU (5) <br/>
_01_03_2022_ - recommender system (16) <br/>
_01_04_2022_ - NLP pretraining, word2vec, GloVe, fastText, BERT (14) <br/>
_01_07_2022_ - modern CNN, AlexNet, VGG, NiN, GoogLeNet, batch normalization, ResNet, DenseNet (7) <br/>
_01_10_2022_ - NLP applications, sentiment analysis, NLI (15) <br/>
_01_12_2022_ - GANs (17) <br/>
_01_13_2022_ - computer vision overview (13) <br/>
_01_14_2022_ - computational performance (12) <br/>
_01_16_2022_ - Practical Machine Learning: Data I, Data II <br/>
_01_17_2022_ - Practical Machine Learning: Data III <br/>
_01_18_2022_ - Practical Machine Learning: ML Model, Model Validation <br/>
_01_19_2022_ - Practical Machine Learning: Complete ðŸš© <br/>

| Chapter |   | Date |
|---------|---|------|
| 1       |   |      | 
| 2       | âœ… |  _12_24_2021_  |
| 3       | âœ… |  _12_30_2021_  |
| 4       | âœ… |  _12_31_2021_  |
| 5       | âœ… |  _01_02_2022_  |
| 6       | âœ… |  _12_25_2021_  |
| 7       | âœ… |  _01_07_2022_  |
| 8       | âœ… |  _12_27_2021_  |
| 9       | âœ… |  _12_28_2021_  |
| 10      | âœ… |  _12_29_2021_  |
| 11      | âœ… |  _01_01_2022_  |
| 12      | âœ… |  _01_14_2022_  |
| 13      | âœ… |  _01_13_2022_  |
| 14      | âœ… |  _01_04_2022_  |
| 15      | âœ… |  _01_10_2022_  |
| 16      | âœ… |  _01_03_2022_  |
| 17      | âœ… |  _01_12_2022_  |
| 18      |   |      |
| 19      |   |      |
