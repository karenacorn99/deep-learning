{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12-ComputationalPerformance.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "kAEE3N5qlrCx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_E0QSlAlohY"
      },
      "outputs": [],
      "source": [
        "!pip install d2l"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib==3.1.3"
      ],
      "metadata": {
        "id": "cR0zKAcElwrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compilers and Interprerters"
      ],
      "metadata": {
        "id": "AKtiCaxClxJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Imperative programming*: makes use of statements to change a program's state"
      ],
      "metadata": {
        "id": "VVo9qrj7mEfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "def fancy_func(a, b, c, d):\n",
        "    e = add(a, b)\n",
        "    f = add(c, d)\n",
        "    g = add(e, f)\n",
        "    return g\n",
        "\n",
        "print(fancy_func(1, 2, 3, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoeJKUK5mhAD",
        "outputId": "f6a9ec40-7101-4b29-d848-2b4a18ec2684"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Symbolic programming:* computation is usually performed only the process has been fully defined.\n",
        "\n",
        "Usual steps:\n",
        "- Define the operations to be be executed\n",
        "- Compile the operations into an executable program\n",
        "- Provide the required inputs and call the compiled program for execution\n",
        "\n",
        "Allowing significant amount of optimization.\n",
        "\n",
        "Python interpreter can be skipped, thus removing the performance bottleneck that can become significant on multiple fast GPUs paired with a single Python thread on a CPU. "
      ],
      "metadata": {
        "id": "6PRHYyGPm7As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_():\n",
        "    return '''\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "'''\n",
        "\n",
        "def fancy_func_():\n",
        "    return '''\n",
        "def fancy_func(a, b, c, d):\n",
        "    e = add(a, b)\n",
        "    f = add(c, d)\n",
        "    g = add(e, f)\n",
        "    return g\n",
        "'''\n",
        "\n",
        "def evoke_():\n",
        "    return add_() + fancy_func_() + 'print(fancy_func(1, 2, 3, 4))'\n",
        "\n",
        "prog = evoke_()\n",
        "print(prog)\n",
        "y = compile(prog, '', 'exec')\n",
        "exec(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPFuLKTKmjc9",
        "outputId": "a09e2c76-a943-4d19-a2d9-564434b4338f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "def add(a, b):\n",
            "    return a + b\n",
            "\n",
            "def fancy_func(a, b, c, d):\n",
            "    e = add(a, b)\n",
            "    f = add(c, d)\n",
            "    g = add(e, f)\n",
            "    return g\n",
            "print(fancy_func(1, 2, 3, 4))\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Differences between *imperative* (interpreted) programming and *symbolic* programming:\n",
        "- Imperative programming is easier to write and debug, because it is easy to obtain and print all relecant intermediate variable values.\n",
        "- Symbolic programming is more efficient and easier to port. Easier to optimize the code during compilation, while also having the ability to port the program into a format independent of Python. Allows program to be run in a non-Python environment, thus avoiding any potential performance issues related to the Python interpreter."
      ],
      "metadata": {
        "id": "wf_ddNaZn6oH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hybrid programming\n",
        "\n",
        "- Tensorflow: symbolic\n",
        "- PyTorch: imperative and ueses dynamic computation graphs"
      ],
      "metadata": {
        "id": "K6GCXVZGortS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hybridizing the Sequential Class"
      ],
      "metadata": {
        "id": "YL1c1i0ApO0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "# Factory for networks\n",
        "def get_net():\n",
        "    net = nn.Sequential(nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2))\n",
        "    return net\n",
        "\n",
        "x = torch.randn(size=(1, 512))\n",
        "net = get_net()\n",
        "net(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y4CSsnNn1qy",
        "outputId": "4256b391-b2d1-41a5-c0f3-e6d9cc4fa6d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0358, -0.0877]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With torch.jit.script, we are able to compile and optimize the computation in the MLP. "
      ],
      "metadata": {
        "id": "lIWOOUPmppVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = torch.jit.script(net)\n",
        "net(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq14bzEXpnly",
        "outputId": "d563b930-724c-467a-eed7-cb058ca04763"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0358, -0.0877]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Benchmark performance"
      ],
      "metadata": {
        "id": "4iQ2NkbCp3xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Benchmark:\n",
        "    \"\"\"For measuring running time.\"\"\"\n",
        "    def __init__(self, description='Done'):\n",
        "        self.description = description\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.timer = d2l.Timer()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        print(f'{self.description}: {self.timer.stop():.4f} sec')"
      ],
      "metadata": {
        "id": "mHrodMe-p1ia"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = get_net()\n",
        "with Benchmark('Without torchscript'):\n",
        "    for i in range(1000): net(x)\n",
        "\n",
        "net = torch.jit.script(net)\n",
        "with Benchmark('With torchscript'):\n",
        "    for i in range(1000): net(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WevGM3Qrp50i",
        "outputId": "24b23919-dbb0-4f96-9b90-741c6e12353d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without torchscript: 0.0939 sec\n",
            "With torchscript: 0.0903 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serialization\n",
        "\n",
        "One benefit of compiling the models is that we can serialize (save) the model and its parameters to disk. This allows us to store a model in a manner that is independent of the front-end language of choice. This allows us to deploy trained model to other devices and easily use other front-end programming languages. At the same time the code is often faster than what can be achieved in imperative programming. "
      ],
      "metadata": {
        "id": "WbDh6cTmqJA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net.save('my_mlp')\n",
        "!ls -lh my_mlp*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te7eqBlSqCYl",
        "outputId": "e0ee80af-3112-43af-a5cf-463cb7c52315"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 651K Jan 14 17:35 my_mlp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Asynchronous Computation"
      ],
      "metadata": {
        "id": "7qOhf9-2q0NI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today's computers are highly parallel systems, consisting of multiple CPU coores (often multiple threads per core), multiple processing elements per GPU, and often multiple GPUs per device.\n",
        "\n",
        "Deep learning frameworks such as MXNet and Tensorflow adopt an aynchronous programming model to improve performance, while PyTorch uses Python's own scheduler leading to a different performance trade-off.\n",
        "\n",
        "For PyTorch, by default, GPU operations are asynchronoous. When you call a function that uses that GPU, the operations are enqueued to the particular device, but not necessarily executed until later. This allows us to execute more computations in parallel, including operations on the CPU or other GPUs.\n",
        "\n",
        "Develop more efficient programs by proactively reducing computational requirements and mutual dependencies. This allows us to reduce memory overhead and incrase processor utilization."
      ],
      "metadata": {
        "id": "bNfcUl0Nry7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import numpy\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "w8ciqH1vqLwU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Asynchronous via Backend"
      ],
      "metadata": {
        "id": "Hy6ICJg_tzQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate a random matrix and multiply it. PyTorch tensor is defined on a GPU."
      ],
      "metadata": {
        "id": "O2KA3N3Nt33z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Warmup for GPU computation\n",
        "device = d2l.try_gpu()\n",
        "a = torch.randn(size=(1000, 1000), device=device)\n",
        "b = torch.mm(a, a)\n",
        "\n",
        "with d2l.Benchmark('numpy'):\n",
        "    for _ in range(10):\n",
        "        a = numpy.random.normal(size=(1000, 1000))\n",
        "        b = numpy.dot(a, a)\n",
        "\n",
        "with d2l.Benchmark('torch'):\n",
        "    for _ in range(10):\n",
        "        a = torch.randn(size=(1000, 1000), device=device)\n",
        "        b = torch.mm(a, a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU0fEjArtyhG",
        "outputId": "ed87745f-5d7d-4891-faf1-f511d7911c62"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy: 1.3420 sec\n",
            "torch: 0.4412 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with d2l.Benchmark():\n",
        "    for _ in range(10):\n",
        "        a = torch.randn(size=(1000, 1000), device=device)\n",
        "        b = torch.mm(a, a)\n",
        "    torch.cuda.synchronize(device)"
      ],
      "metadata": {
        "id": "78SCBbu0tywv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependency graph"
      ],
      "metadata": {
        "id": "3C3P8L0vukNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones((1, 2), device=device)\n",
        "y = torch.ones((1, 2), device=device)\n",
        "z = x * y + 2\n",
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqiJYuCEuNL2",
        "outputId": "0699bbae-e449-4b3a-9a1e-b99ef6757766"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3., 3.]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**:\n",
        "- Deep learning framework may decouple the Python frontend from an execution backend. This allows for fast asynchronous insertion of commands into the backend and associated parallellism\n",
        "- Asynchrony leads to a rather responsive frontend. However, use caution not to overfill the task queue since it may lead to excessive memory consumption. It is recommended to synchronize for each minibatch to keep frontend and backend approximately synchronized. \n",
        "- Chip vendors offer sophiticated performance analysis tools to obtain a much more fine-grained insight into the efficiency of deep learning."
      ],
      "metadata": {
        "id": "0-FDMvujvHxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic Parallelism\n",
        "\n",
        "Deep learrning frameworks automatically construct computational graphs at the backend. Using a computational graph, the system is aware of all the dependencies, can selectively execute multiple non-interdependent tasks in parallel to improve speed."
      ],
      "metadata": {
        "id": "c2xNtLXYv3YC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hardware"
      ],
      "metadata": {
        "id": "bY-p0MoPfACR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**:\n",
        "- Devices have overheads for operations. Hence, it is important to aim for a small number of large transfers rather than many small ones. This applies to RAM, SSDs, networks and GPUs.\n",
        "- Vectorization is key for performance. Make sure you are aware of the specific abilities of your accelerators. \n",
        "- Numerical overflow due to small data types can be a probllem during training (and to a lesser extent during inference)\n",
        "- Aliasing can significantly degrade performance.\n",
        "- Match your algorithm to the hardware. Great speedup can be achieved when fitting the parameters into caches.\n",
        "- It is recommended to sketch out performance of a novel algorithm on paper before verifying the experimentall results. Discrepencies of an order-of-magnitude or more are reasons for concern.\n",
        "- Use profilers to debug performance bottlenecks.\n",
        "- Training and inference hardware have different sweet spots in terms of price and performance."
      ],
      "metadata": {
        "id": "7qv5kWpCyxml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training on Multiple GPUs\n",
        "\n",
        "By and large, data parallelism is the most convenient way to proceed."
      ],
      "metadata": {
        "id": "zPqkmI30z5-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**:\n",
        "- There are multiple ways to split deep network training over multple GPUs. We could split them between layers, across layers, or across data. The former two require tightly choreographed data transfers. Data parallelism is the simplest strategy.\n",
        "- Data parallel training is straightforward. However, it increases the effective minibatch size to be efficient.\n",
        "- In data parallelism, data are split across multiple GPUs, where each GPU executes its own forward and backward operation and subsequently gradients are aggregated and results are braodcast back to the GPUs. \n",
        "- We may use slightly increased learning rates for larger minibatches."
      ],
      "metadata": {
        "id": "1vMIkrn32EWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter Servers"
      ],
      "metadata": {
        "id": "1xjyxED24e-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**:\n",
        "- Synchronization needs to be highly adaptive to specific network infrastructure and connectivity within a server. This can make a significant different to the time it takes to synchronize.\n",
        "- Ring-synchronization can be optimal for p3 and DGX-2 servers. For others possibly not so much.\n",
        "- A hierarchical synchronization strategy works well when adding multiple parameter servers for increased bandwidth."
      ],
      "metadata": {
        "id": "cbw3pUtB4gkq"
      }
    }
  ]
}